{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It contains all operations for building a machine learning model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing files/data from analysis.ipynb file for creating model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "[nltk_data] Downloading package punkt to\n[nltk_data]     C:\\Users\\TUSHAR\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n"
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "nltk.download('punkt')\n",
    "import import_ipynb\n",
    "pd.set_option('display.max_rows', None)\n",
    "from analysis import data,store_tag\n",
    "import pickle\n",
    "import requests\n",
    "import numpy as np\n",
    "import joblib "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                           repo_name   star   fork  watch  \\\n0                                              keras  47900  18100   2100   \n1                                       scikit-learn  40300  19600   2200   \n2                          PythonDataScienceHandbook  23100   9900   1500   \n3  Probabilistic-Programming-and-Bayesian-Methods...  21000   6600   1400   \n4                          Data-Science--Cheat-Sheet  18400   8200   1500   \n\n   issue                                               tags  \\\n0   2940  ['deep-learning', 'tensorflow', 'neural-networ...   \n1   1505  ['machine-learning', 'python', 'statistics', '...   \n2     65  ['scikit-learn', 'numpy', 'python', 'jupyter-n...   \n3    127  ['bayesian-methods', 'pymc', 'mathematical-ana...   \n4      7                                                 []   \n\n                                         description  contributers  license  \\\n0                           deep learning for humans          49.0        1   \n1           scikit learn  machine learning in python         108.0        1   \n2  python data science handbook  full text in jup...           0.0        1   \n3  aka  bayesian methods for hackers   an introdu...           0.0        1   \n4                                       cheat sheets           0.0        0   \n\n                                            repo_url    most_used_lang  \\\n0                https://github.com/keras-team/keras            python   \n1       https://github.com/scikit-learn/scikit-learn            python   \n2  https://github.com/jakevdp/PythonDataScienceHa...  jupyter notebook   \n3  https://github.com/CamDavidsonPilon/Probabilis...  jupyter notebook   \n4  https://github.com/abhat222/Data-Science--Chea...       no language   \n\n   tag_ratio                                          repo_desc  \\\n0       9.44                           deep learning for humans   \n1       6.05           scikit learn  machine learning in python   \n2       2.46  python data science handbook  full text in jup...   \n3       1.75  aka  bayesian methods for hackers   an introdu...   \n4       0.00                                       cheat sheets   \n\n              language  desc_to_num  repo_to_num  \n0             [python]          571           73  \n1             [python]          307           33  \n2  [jupyter, notebook]          859            0  \n3  [jupyter, notebook]         6863            0  \n4       [no, language]          902            0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>repo_name</th>\n      <th>star</th>\n      <th>fork</th>\n      <th>watch</th>\n      <th>issue</th>\n      <th>tags</th>\n      <th>description</th>\n      <th>contributers</th>\n      <th>license</th>\n      <th>repo_url</th>\n      <th>most_used_lang</th>\n      <th>tag_ratio</th>\n      <th>repo_desc</th>\n      <th>language</th>\n      <th>desc_to_num</th>\n      <th>repo_to_num</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>keras</td>\n      <td>47900</td>\n      <td>18100</td>\n      <td>2100</td>\n      <td>2940</td>\n      <td>['deep-learning', 'tensorflow', 'neural-networ...</td>\n      <td>deep learning for humans</td>\n      <td>49.0</td>\n      <td>1</td>\n      <td>https://github.com/keras-team/keras</td>\n      <td>python</td>\n      <td>9.44</td>\n      <td>deep learning for humans</td>\n      <td>[python]</td>\n      <td>571</td>\n      <td>73</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>scikit-learn</td>\n      <td>40300</td>\n      <td>19600</td>\n      <td>2200</td>\n      <td>1505</td>\n      <td>['machine-learning', 'python', 'statistics', '...</td>\n      <td>scikit learn  machine learning in python</td>\n      <td>108.0</td>\n      <td>1</td>\n      <td>https://github.com/scikit-learn/scikit-learn</td>\n      <td>python</td>\n      <td>6.05</td>\n      <td>scikit learn  machine learning in python</td>\n      <td>[python]</td>\n      <td>307</td>\n      <td>33</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>PythonDataScienceHandbook</td>\n      <td>23100</td>\n      <td>9900</td>\n      <td>1500</td>\n      <td>65</td>\n      <td>['scikit-learn', 'numpy', 'python', 'jupyter-n...</td>\n      <td>python data science handbook  full text in jup...</td>\n      <td>0.0</td>\n      <td>1</td>\n      <td>https://github.com/jakevdp/PythonDataScienceHa...</td>\n      <td>jupyter notebook</td>\n      <td>2.46</td>\n      <td>python data science handbook  full text in jup...</td>\n      <td>[jupyter, notebook]</td>\n      <td>859</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Probabilistic-Programming-and-Bayesian-Methods...</td>\n      <td>21000</td>\n      <td>6600</td>\n      <td>1400</td>\n      <td>127</td>\n      <td>['bayesian-methods', 'pymc', 'mathematical-ana...</td>\n      <td>aka  bayesian methods for hackers   an introdu...</td>\n      <td>0.0</td>\n      <td>1</td>\n      <td>https://github.com/CamDavidsonPilon/Probabilis...</td>\n      <td>jupyter notebook</td>\n      <td>1.75</td>\n      <td>aka  bayesian methods for hackers   an introdu...</td>\n      <td>[jupyter, notebook]</td>\n      <td>6863</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Data-Science--Cheat-Sheet</td>\n      <td>18400</td>\n      <td>8200</td>\n      <td>1500</td>\n      <td>7</td>\n      <td>[]</td>\n      <td>cheat sheets</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>https://github.com/abhat222/Data-Science--Chea...</td>\n      <td>no language</td>\n      <td>0.00</td>\n      <td>cheat sheets</td>\n      <td>[no, language]</td>\n      <td>902</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 103
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description is also play a role in the popularity of your repos, so we need to clean  `description` tuple by removing punctuations and creating word counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['repo_desc']=data['description']\n",
    "data['description']=data['description'].str.replace('\\W',\" \")\n",
    "data['description']=data['description'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['most_used_lang']=data['most_used_lang'].str.replace('\\W',\" \")\n",
    "data['most_used_lang']=data['most_used_lang'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['language']=data['most_used_lang'].str.split()\n",
    "vocabulary = []\n",
    "for words in data['language']:\n",
    "    for word in words:\n",
    "        vocabulary.append(word)\n",
    "        \n",
    "vocabulary = list(set(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts_for_language = {unique_word: [0] * len(data['language']) for unique_word in vocabulary}\n",
    "\n",
    "for index, words in enumerate(data['language']):\n",
    "    for word in words:\n",
    "        word_counts_for_language[word][index] += 1\n",
    "\n",
    "language_col=pd.DataFrame(word_counts_for_language)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing description "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=[]\n",
    "for index,row in data.iterrows():\n",
    "    tokens=nltk.word_tokenize(row['description'])\n",
    "    text.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopword=stopwords.words('english')"
   ]
  },
  {
   "source": [
    "To get numerical form of description , we create word dict according to their frequency i.e. the word which occurs more would be have low index value and removing words which has less `freq`(threshold=1)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def txt_to_num(text, threshold_rarewords = 1):\n",
    "    \n",
    "    # removing sublist from text list\n",
    "    if len(text) > 1:\n",
    "        text_list = [item for sublist in text for item in sublist]\n",
    "    else:\n",
    "        text_list = text\n",
    "    \n",
    "    # get word freuqncy\n",
    "    freq_word = nltk.FreqDist(text_list)\n",
    "\n",
    "\n",
    "    df = pd.DataFrame.from_dict(freq_word, orient='index')\n",
    "    df.columns = ['freq']\n",
    "      \n",
    "\n",
    "    df.sort_values(by=['freq'], ascending=False, inplace=True)\n",
    "  \n",
    "    # Add word index\n",
    "    number_of_words = df.shape[0]\n",
    "    for word in df.index:\n",
    "        if word in stopword:\n",
    "            df.loc[word]['freq']=0\n",
    "    df.sort_values('freq',inplace=True,ascending=False)\n",
    "    \n",
    "    df['word_index'] = list(np.arange(number_of_words)+1)\n",
    "    \n",
    "    # indexing 0 for rare words \n",
    "    frequency = df['freq'].values\n",
    "    word_index = df['word_index'].values\n",
    "    mask = frequency <= threshold_rarewords\n",
    "    word_index[mask] = 0\n",
    "    df['word_index'] =  word_index\n",
    "    \n",
    "\n",
    "    word_dict = df['word_index'].to_dict()\n",
    "\n",
    "    #dict for word into num\n",
    "    text_num = []\n",
    "    for string in text:\n",
    "        string_numbers = [word_dict[word] for word in string]\n",
    "        text_num.append(string_numbers)  \n",
    "    \n",
    "    return (text_num,word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "desc_num,word_dict=txt_to_num(text)\n",
    "txt=[sum(x)  for x in desc_num]\n",
    "data['desc_to_num']=txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_text=[]\n",
    "for index,row in data.iterrows():\n",
    "    tokens=nltk.word_tokenize(row['repo_name'])\n",
    "    repo_text.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "repo_num,word_dict=txt_to_num(repo_text)\n",
    "txt=[sum(x)  for x in repo_num]\n",
    "data['repo_to_num']=txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                           repo_name   star   fork  watch  \\\n0                                              keras  47900  18100   2100   \n1                                       scikit-learn  40300  19600   2200   \n2                          PythonDataScienceHandbook  23100   9900   1500   \n3  Probabilistic-Programming-and-Bayesian-Methods...  21000   6600   1400   \n4                          Data-Science--Cheat-Sheet  18400   8200   1500   \n\n   issue                                               tags  \\\n0   2940  ['deep-learning', 'tensorflow', 'neural-networ...   \n1   1505  ['machine-learning', 'python', 'statistics', '...   \n2     65  ['scikit-learn', 'numpy', 'python', 'jupyter-n...   \n3    127  ['bayesian-methods', 'pymc', 'mathematical-ana...   \n4      7                                                 []   \n\n                                         description  contributers  license  \\\n0                           deep learning for humans          49.0        1   \n1           scikit learn  machine learning in python         108.0        1   \n2  python data science handbook  full text in jup...           0.0        1   \n3  aka  bayesian methods for hackers   an introdu...           0.0        1   \n4                                       cheat sheets           0.0        0   \n\n                                            repo_url    most_used_lang  \\\n0                https://github.com/keras-team/keras            python   \n1       https://github.com/scikit-learn/scikit-learn            python   \n2  https://github.com/jakevdp/PythonDataScienceHa...  jupyter notebook   \n3  https://github.com/CamDavidsonPilon/Probabilis...  jupyter notebook   \n4  https://github.com/abhat222/Data-Science--Chea...       no language   \n\n   tag_ratio                                          repo_desc  \\\n0       9.44                           deep learning for humans   \n1       6.05           scikit learn  machine learning in python   \n2       2.46  python data science handbook  full text in jup...   \n3       1.75  aka  bayesian methods for hackers   an introdu...   \n4       0.00                                       cheat sheets   \n\n              language  desc_to_num  repo_to_num  \n0             [python]          571           73  \n1             [python]          307           33  \n2  [jupyter, notebook]          859            0  \n3  [jupyter, notebook]         6863            0  \n4       [no, language]          902            0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>repo_name</th>\n      <th>star</th>\n      <th>fork</th>\n      <th>watch</th>\n      <th>issue</th>\n      <th>tags</th>\n      <th>description</th>\n      <th>contributers</th>\n      <th>license</th>\n      <th>repo_url</th>\n      <th>most_used_lang</th>\n      <th>tag_ratio</th>\n      <th>repo_desc</th>\n      <th>language</th>\n      <th>desc_to_num</th>\n      <th>repo_to_num</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>keras</td>\n      <td>47900</td>\n      <td>18100</td>\n      <td>2100</td>\n      <td>2940</td>\n      <td>['deep-learning', 'tensorflow', 'neural-networ...</td>\n      <td>deep learning for humans</td>\n      <td>49.0</td>\n      <td>1</td>\n      <td>https://github.com/keras-team/keras</td>\n      <td>python</td>\n      <td>9.44</td>\n      <td>deep learning for humans</td>\n      <td>[python]</td>\n      <td>571</td>\n      <td>73</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>scikit-learn</td>\n      <td>40300</td>\n      <td>19600</td>\n      <td>2200</td>\n      <td>1505</td>\n      <td>['machine-learning', 'python', 'statistics', '...</td>\n      <td>scikit learn  machine learning in python</td>\n      <td>108.0</td>\n      <td>1</td>\n      <td>https://github.com/scikit-learn/scikit-learn</td>\n      <td>python</td>\n      <td>6.05</td>\n      <td>scikit learn  machine learning in python</td>\n      <td>[python]</td>\n      <td>307</td>\n      <td>33</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>PythonDataScienceHandbook</td>\n      <td>23100</td>\n      <td>9900</td>\n      <td>1500</td>\n      <td>65</td>\n      <td>['scikit-learn', 'numpy', 'python', 'jupyter-n...</td>\n      <td>python data science handbook  full text in jup...</td>\n      <td>0.0</td>\n      <td>1</td>\n      <td>https://github.com/jakevdp/PythonDataScienceHa...</td>\n      <td>jupyter notebook</td>\n      <td>2.46</td>\n      <td>python data science handbook  full text in jup...</td>\n      <td>[jupyter, notebook]</td>\n      <td>859</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Probabilistic-Programming-and-Bayesian-Methods...</td>\n      <td>21000</td>\n      <td>6600</td>\n      <td>1400</td>\n      <td>127</td>\n      <td>['bayesian-methods', 'pymc', 'mathematical-ana...</td>\n      <td>aka  bayesian methods for hackers   an introdu...</td>\n      <td>0.0</td>\n      <td>1</td>\n      <td>https://github.com/CamDavidsonPilon/Probabilis...</td>\n      <td>jupyter notebook</td>\n      <td>1.75</td>\n      <td>aka  bayesian methods for hackers   an introdu...</td>\n      <td>[jupyter, notebook]</td>\n      <td>6863</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Data-Science--Cheat-Sheet</td>\n      <td>18400</td>\n      <td>8200</td>\n      <td>1500</td>\n      <td>7</td>\n      <td>[]</td>\n      <td>cheat sheets</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>https://github.com/abhat222/Data-Science--Chea...</td>\n      <td>no language</td>\n      <td>0.00</td>\n      <td>cheat sheets</td>\n      <td>[no, language]</td>\n      <td>902</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 114
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above we can see that their are some numerical values and stopwords too, so we need to remove them from vocabulary list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Index(['repo_name', 'star', 'fork', 'watch', 'issue', 'tags', 'description',\n       'contributers', 'license', 'repo_url', 'most_used_lang', 'tag_ratio',\n       'repo_desc', 'language', 'desc_to_num', 'repo_to_num', 'purebasic',\n       'pure', 'vim', 'language', 'go', 'hcl', 'elixir', 'script', 'haskell',\n       'actionscript', 'objective', 'kotlin', 'rust', 'notebook', 'jsonnet',\n       'html', 'css', 'javascript', 'processing', 'freemarker', 'dart',\n       'coffeescript', 'c', 'swift', 'lua', 'vue', 'ruby', 'ocaml', 'clojure',\n       'php', 'powershell', 'xslt', 'dockerfile', 'data', 'tex', 'shell',\n       'roff', 'groovy', 'perl', 'java', 'no', 'other', 'eagle', 'pascal',\n       'julia', 'scala', 'python', 'makefile', 'matlab', 'qml', 'assembly',\n       'shaderlab', 'typescript', 'r', 'jupyter'],\n      dtype='object')"
     },
     "metadata": {},
     "execution_count": 115
    }
   ],
   "source": [
    "training_set=pd.concat([data,language_col],axis=1)\n",
    "training_set.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set.drop(['repo_name','tags','description','repo_url','language','no','repo_desc','most_used_lang'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_set = training_set.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LR can be used:\n",
    "# gradient descent\n",
    "# reandom forest \n",
    "# NN regression (Relu)\n",
    "# Lasso regression\n",
    "# Ridge regression\n",
    "# Elastic netregression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_set.iloc[:,1:],training_set.iloc[:,0], test_size=0.2, random_state=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "9022353906721058.0\nR2 score:  -3.144698354181925e+23\n"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "sgd=SGDRegressor(loss='squared_loss',alpha=0.001,max_iter=3500)\n",
    "sgd.fit(X_train, y_train)\n",
    "predictions = sgd.predict(X_test)\n",
    "mse = mean_squared_error(y_test, predictions, squared=False)\n",
    "print(mse)\n",
    "accu=r2_score(y_test, predictions)\n",
    "print(\"R2 score: \",accu)\n",
    "if accu>ac:\n",
    "    model=sgd\n",
    "    ac=accu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normal Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "5678.176677164683\nR2 score:  0.8754461720547139\n"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "predictions = lr.predict(X_test)\n",
    "mse = mean_squared_error(y_test, predictions, squared=False)\n",
    "print(mse)\n",
    "accu=r2_score(y_test, predictions)\n",
    "print(\"R2 score: \",accu)\n",
    "if accu>ac:\n",
    "    model=lr\n",
    "    ac=accu"
   ]
  },
  {
   "source": [
    "Random forest"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "5016.828342116424\nR2 score:  0.9027705657167036\n"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rf = RandomForestRegressor(max_depth=20, random_state=1)\n",
    "rf.fit(X_train, y_train)\n",
    "predictions = rf.predict(X_test)\n",
    "mse = mean_squared_error(y_test, predictions, squared=False)\n",
    "print(mse)\n",
    "accu=r2_score(y_test, predictions)\n",
    "print(\"R2 score: \",accu)\n",
    "if accu>ac:\n",
    "    model=rf\n",
    "    ac=accu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from numpy import loadtxt\n",
    "# from keras import Sequential\n",
    "# from keras.layers import Dense\n",
    "# X = training_set.iloc[:,1:]\n",
    "# y = training_set['star']\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Dense(8, input_dim=8, activation='relu'))\n",
    "# model.add(Dense(1, activation='relu'))\n",
    "\n",
    "\n",
    "# model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# model.fit(X, y, epochs=150)\n",
    "\n",
    "# _, accuracy = model.evaluate(X, y)\n",
    "# print('Accuracy: %.2f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "5678.1074368604695\nR2 score:  0.8754492096820532\n"
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "las = linear_model.Lasso(alpha=0.01,max_iter=2000)\n",
    "las.fit(X_train, y_train)\n",
    "predictions = las.predict(X_test)\n",
    "mse = mean_squared_error(y_test, predictions, squared=False)\n",
    "print(mse)\n",
    "accu=r2_score(y_test, predictions)\n",
    "print(\"R2 score: \",accu)\n",
    "if accu>ac:\n",
    "    model=las\n",
    "    ac=accu"
   ]
  },
  {
   "source": [
    "Ridge"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "5678.210996840213\nR2 score:  0.8754446664094611\n"
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "las = linear_model.Ridge(alpha=0.001,max_iter=3000)\n",
    "las.fit(X_train, y_train)\n",
    "predictions = las.predict(X_test)\n",
    "mse = mean_squared_error(y_test, predictions, squared=False)\n",
    "print(mse)\n",
    "accu=r2_score(y_test, predictions)\n",
    "print(\"R2 score: \",accu)\n",
    "if accu>ac:\n",
    "    model=las\n",
    "    ac=accu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elastic netregression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "6014.34626266683\nR2 score:  0.8602614822115334\n"
    }
   ],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "elsnr = ElasticNet(alpha=0.1,max_iter=3000)\n",
    "elsnr.fit(X_train, y_train)\n",
    "predictions = elsnr.predict(X_test)\n",
    "mse = mean_squared_error(y_test, predictions, squared=False)\n",
    "print(mse)\n",
    "accu=r2_score(y_test, predictions)\n",
    "print(\"R2 score: \",accu)\n",
    "if accu>ac:\n",
    "    model=elsnr\n",
    "    ac=accu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_model.pkl', 'wb') as f:  \n",
    "    pickle.dump(model, f)\n",
    "with open('objs.pkl', 'wb') as f:  \n",
    "    pickle.dump([store_tag,vocabulary,word_dict], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_name=input(\"Enter username:\")\n",
    "repo_name=input(\"enter repo name:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyError",
     "evalue": "'forks'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-130-7794da6e40f7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mkeys\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'forks'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'watchers'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'open_issues_count'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'topics'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'description'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'language'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mdataf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkeys\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'license'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'none'\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'license'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mlicense\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-130-7794da6e40f7>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mkeys\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'forks'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'watchers'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'open_issues_count'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'topics'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'description'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'language'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mdataf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkeys\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'license'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'none'\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'license'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mlicense\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'forks'"
     ]
    }
   ],
   "source": [
    "header={'Accept':'application/vnd.github.mercy-preview+json',\n",
    "'visibility':'PUBLIC'\n",
    "}\n",
    "request=requests.get('https://api.github.com/repos/'+user_name+'/'+repo_name,headers=header)\n",
    "data=request.json()\n",
    "keys=['forks','watchers','open_issues_count','topics','description','language']\n",
    "dataf=[data[k] for k in keys]\n",
    "if str(data['license']).lower()=='none' or data['license']:\n",
    "    license=0\n",
    "else:\n",
    "    license=1\n",
    "\n",
    "request=requests.get('https://api.github.com/repos/'+user_name+'/'+repo_name+'/contributors',headers=header)\n",
    "dat=request.json()\n",
    "contributors=len(dat)\n",
    "dataf.append(contributors)\n",
    "dataf.append(license)\n",
    "df=pd.DataFrame([dataf],columns=['fork','watch','issue','tags','description','most_used_lang','contributers','license'])\n",
    "def perc(tags):\n",
    "    sum_of_perc=0\n",
    "    for tag in tags:\n",
    "        if tag in store_tag:\n",
    "            sum_of_perc+=(store_tag[tag]/vocabulary)\n",
    "   \n",
    "    return (sum_of_perc*100)\n",
    "\n",
    "df['tag_ratio']=df['tags'].apply(perc).astype(float)\n",
    "text=nltk.word_tokenize(df['description'][0])\n",
    "tex=nltk.word_tokenize(repo_name)\n",
    "def string_num(text):\n",
    "    string_numbers=0\n",
    "    for string in text:\n",
    "        if string in word_dict:\n",
    "            string_numbers+=word_dict[string]\n",
    "    return string_numbers\n",
    "df['desc_to_num']=string_num(text)\n",
    "df['repo_to_num']=string_num(tex)\n",
    "word_counts_for_language = {unique_word: [0] for unique_word in vocabulary}\n",
    "df['most_used_lang']=df['most_used_lang'].str.split()\n",
    "for word in df['most_used_lang']:\n",
    "    if word[0] in word_counts_for_language:\n",
    "        word_counts_for_language[word.lower()][index] += 1\n",
    "\n",
    "language_col=pd.DataFrame(word_counts_for_language)\n",
    "test_set=pd.concat([df,language_col],axis=1)\n",
    "test_set.drop(['tags','description','language','no','most_used_lang'],axis=1,inplace=True)\n",
    "model=pickle.load(open('train_model.pkl','rb'))\n",
    "print(\"your repo Popularity:\",model.predict(test_set)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('base': conda)",
   "language": "python",
   "name": "python_defaultSpec_1599952711006"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}