{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we use the processed data from the `analysis.ipynb` file and applied the various methods on it to find the better r2_score. Saved the final model in pickle format for the prediction and at last done the prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing files/data from analysis.ipynb file for creating model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'import_ipynb'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjoblib\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mimport_ipynb\u001b[39;00m\n\u001b[0;32m     10\u001b[0m nltk\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpunkt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     11\u001b[0m pd\u001b[38;5;241m.\u001b[39mset_option(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdisplay.max_rows\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'import_ipynb'"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import requests\n",
    "import numpy as np\n",
    "import joblib\n",
    "import import_ipynb\n",
    "\n",
    "nltk.download('punkt')\n",
    "pd.set_option('display.max_rows', None)\n",
    "%run analysis.ipynb import data, store_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description is also play a role in the popularity of your repos, so we need to clean  `description` tuple by removing punctuations and creating word counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data['description']=data['description'].str.replace('\\W',\" \")\n",
    "data['description']=data['description'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['most_used_lang']=data['most_used_lang'].str.replace('\\W',\" \")\n",
    "data['most_used_lang']=data['most_used_lang'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['language']=data['most_used_lang'].str.split()\n",
    "vocab = []\n",
    "for words in data['language']:\n",
    "    for word in words:\n",
    "        vocab.append(word)\n",
    "        \n",
    "        \n",
    "vocab = list(set(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>repo_name</th>\n",
       "      <th>star</th>\n",
       "      <th>fork</th>\n",
       "      <th>watch</th>\n",
       "      <th>issue</th>\n",
       "      <th>tags</th>\n",
       "      <th>description</th>\n",
       "      <th>contributers</th>\n",
       "      <th>license</th>\n",
       "      <th>repo_url</th>\n",
       "      <th>most_used_lang</th>\n",
       "      <th>tag_ratio</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>keras</td>\n",
       "      <td>47900</td>\n",
       "      <td>18100</td>\n",
       "      <td>2100</td>\n",
       "      <td>2940</td>\n",
       "      <td>['deep-learning', 'tensorflow', 'neural-networ...</td>\n",
       "      <td>deep learning for humans</td>\n",
       "      <td>49.0</td>\n",
       "      <td>1</td>\n",
       "      <td>https://github.com/keras-team/keras</td>\n",
       "      <td>python</td>\n",
       "      <td>9.31</td>\n",
       "      <td>[python]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>scikit-learn</td>\n",
       "      <td>40300</td>\n",
       "      <td>19600</td>\n",
       "      <td>2200</td>\n",
       "      <td>1505</td>\n",
       "      <td>['machine-learning', 'python', 'statistics', '...</td>\n",
       "      <td>scikit learn  machine learning in python</td>\n",
       "      <td>108.0</td>\n",
       "      <td>1</td>\n",
       "      <td>https://github.com/scikit-learn/scikit-learn</td>\n",
       "      <td>python</td>\n",
       "      <td>5.97</td>\n",
       "      <td>[python]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PythonDataScienceHandbook</td>\n",
       "      <td>23100</td>\n",
       "      <td>9900</td>\n",
       "      <td>1500</td>\n",
       "      <td>65</td>\n",
       "      <td>['scikit-learn', 'numpy', 'python', 'jupyter-n...</td>\n",
       "      <td>python data science handbook  full text in jup...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>https://github.com/jakevdp/PythonDataScienceHa...</td>\n",
       "      <td>jupyter notebook</td>\n",
       "      <td>2.42</td>\n",
       "      <td>[jupyter, notebook]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Probabilistic-Programming-and-Bayesian-Methods...</td>\n",
       "      <td>21000</td>\n",
       "      <td>6600</td>\n",
       "      <td>1400</td>\n",
       "      <td>127</td>\n",
       "      <td>['bayesian-methods', 'pymc', 'mathematical-ana...</td>\n",
       "      <td>aka  bayesian methods for hackers   an introdu...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>https://github.com/CamDavidsonPilon/Probabilis...</td>\n",
       "      <td>jupyter notebook</td>\n",
       "      <td>1.72</td>\n",
       "      <td>[jupyter, notebook]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data-Science--Cheat-Sheet</td>\n",
       "      <td>18400</td>\n",
       "      <td>8200</td>\n",
       "      <td>1500</td>\n",
       "      <td>7</td>\n",
       "      <td>[]</td>\n",
       "      <td>cheat sheets</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://github.com/abhat222/Data-Science--Chea...</td>\n",
       "      <td>no language</td>\n",
       "      <td>0.00</td>\n",
       "      <td>[no, language]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           repo_name   star   fork  watch  \\\n",
       "0                                              keras  47900  18100   2100   \n",
       "1                                       scikit-learn  40300  19600   2200   \n",
       "2                          PythonDataScienceHandbook  23100   9900   1500   \n",
       "3  Probabilistic-Programming-and-Bayesian-Methods...  21000   6600   1400   \n",
       "4                          Data-Science--Cheat-Sheet  18400   8200   1500   \n",
       "\n",
       "   issue                                               tags  \\\n",
       "0   2940  ['deep-learning', 'tensorflow', 'neural-networ...   \n",
       "1   1505  ['machine-learning', 'python', 'statistics', '...   \n",
       "2     65  ['scikit-learn', 'numpy', 'python', 'jupyter-n...   \n",
       "3    127  ['bayesian-methods', 'pymc', 'mathematical-ana...   \n",
       "4      7                                                 []   \n",
       "\n",
       "                                         description  contributers  license  \\\n",
       "0                           deep learning for humans          49.0        1   \n",
       "1           scikit learn  machine learning in python         108.0        1   \n",
       "2  python data science handbook  full text in jup...           0.0        1   \n",
       "3  aka  bayesian methods for hackers   an introdu...           0.0        1   \n",
       "4                                       cheat sheets           0.0        0   \n",
       "\n",
       "                                            repo_url    most_used_lang  \\\n",
       "0                https://github.com/keras-team/keras            python   \n",
       "1       https://github.com/scikit-learn/scikit-learn            python   \n",
       "2  https://github.com/jakevdp/PythonDataScienceHa...  jupyter notebook   \n",
       "3  https://github.com/CamDavidsonPilon/Probabilis...  jupyter notebook   \n",
       "4  https://github.com/abhat222/Data-Science--Chea...       no language   \n",
       "\n",
       "   tag_ratio             language  \n",
       "0       9.31             [python]  \n",
       "1       5.97             [python]  \n",
       "2       2.42  [jupyter, notebook]  \n",
       "3       1.72  [jupyter, notebook]  \n",
       "4       0.00       [no, language]  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts_for_language = {unique_word: [0] * len(data['language']) for unique_word in vocab}\n",
    "\n",
    "for index, words in enumerate(data['language']):\n",
    "    for word in words:\n",
    "        word_counts_for_language[word][index] += 1\n",
    "\n",
    "language_col=pd.DataFrame(word_counts_for_language)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing the text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=[]\n",
    "for index,row in data.iterrows():\n",
    "    tokens=nltk.word_tokenize(row['description'])\n",
    "    text.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/myth/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stopword=stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get numerical form of description , we create word dict according to their frequency i.e. the word which occurs more would be have low index value and removing words which has less `freq`(threshold=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def txt_to_num(text, threshold_rarewords = 1):\n",
    "    \n",
    "    # removing sublist from text list\n",
    "    if len(text) > 1:\n",
    "        text_list = [item for sublist in text for item in sublist]\n",
    "    else:\n",
    "        text_list = text\n",
    "    \n",
    "    # get word freuqncy\n",
    "    freq_word = nltk.FreqDist(text_list)\n",
    "\n",
    "\n",
    "    df = pd.DataFrame.from_dict(freq_word, orient='index')\n",
    "    df.columns = ['freq']\n",
    "      \n",
    "\n",
    "    df.sort_values(by=['freq'], ascending=False, inplace=True)\n",
    "  \n",
    "    # Add word index\n",
    "    number_of_words = df.shape[0]\n",
    "    for word in df.index:\n",
    "        if word in stopword:\n",
    "            df.loc[word]['freq']=0\n",
    "    df.sort_values('freq',inplace=True,ascending=False)\n",
    "    \n",
    "    df['word_index'] = list(np.arange(number_of_words)+1)\n",
    "    \n",
    "    # indexing 0 for rare words \n",
    "    frequency = df['freq'].values\n",
    "    word_index = df['word_index'].values\n",
    "    mask = frequency <= threshold_rarewords\n",
    "    word_index[mask] = 0\n",
    "    df['word_index'] =  word_index\n",
    "    \n",
    "    \n",
    "    word_dict = df['word_index'].to_dict()\n",
    "    # print(df.tail())\n",
    "    # dict for word into num\n",
    "    text_num = []\n",
    "    for string in text:\n",
    "        string_numbers = [word_dict[word] for word in string]\n",
    "        text_num.append(string_numbers)  \n",
    "    \n",
    "    return (text_num, word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "desc_num,word_dict=txt_to_num(text)\n",
    "txt=[sum(x)  for x in desc_num]\n",
    "data['desc_to_num']=txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repo_text=[]\n",
    "# for index,row in data.iterrows():\n",
    "#     tokens=nltk.word_tokenize(row['repo_name'])\n",
    "#     repo_text.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# repo_num,repo_word_dict=txt_to_num(repo_text)\n",
    "# txt=[sum(x)  for x in repo_num]\n",
    "# data['repo_to_num']=txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>repo_name</th>\n",
       "      <th>star</th>\n",
       "      <th>fork</th>\n",
       "      <th>watch</th>\n",
       "      <th>issue</th>\n",
       "      <th>tags</th>\n",
       "      <th>description</th>\n",
       "      <th>contributers</th>\n",
       "      <th>license</th>\n",
       "      <th>repo_url</th>\n",
       "      <th>most_used_lang</th>\n",
       "      <th>tag_ratio</th>\n",
       "      <th>language</th>\n",
       "      <th>desc_to_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>keras</td>\n",
       "      <td>47900</td>\n",
       "      <td>18100</td>\n",
       "      <td>2100</td>\n",
       "      <td>2940</td>\n",
       "      <td>['deep-learning', 'tensorflow', 'neural-networ...</td>\n",
       "      <td>deep learning for humans</td>\n",
       "      <td>49.0</td>\n",
       "      <td>1</td>\n",
       "      <td>https://github.com/keras-team/keras</td>\n",
       "      <td>python</td>\n",
       "      <td>9.31</td>\n",
       "      <td>[python]</td>\n",
       "      <td>737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>scikit-learn</td>\n",
       "      <td>40300</td>\n",
       "      <td>19600</td>\n",
       "      <td>2200</td>\n",
       "      <td>1505</td>\n",
       "      <td>['machine-learning', 'python', 'statistics', '...</td>\n",
       "      <td>scikit learn  machine learning in python</td>\n",
       "      <td>108.0</td>\n",
       "      <td>1</td>\n",
       "      <td>https://github.com/scikit-learn/scikit-learn</td>\n",
       "      <td>python</td>\n",
       "      <td>5.97</td>\n",
       "      <td>[python]</td>\n",
       "      <td>477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PythonDataScienceHandbook</td>\n",
       "      <td>23100</td>\n",
       "      <td>9900</td>\n",
       "      <td>1500</td>\n",
       "      <td>65</td>\n",
       "      <td>['scikit-learn', 'numpy', 'python', 'jupyter-n...</td>\n",
       "      <td>python data science handbook  full text in jup...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>https://github.com/jakevdp/PythonDataScienceHa...</td>\n",
       "      <td>jupyter notebook</td>\n",
       "      <td>2.42</td>\n",
       "      <td>[jupyter, notebook]</td>\n",
       "      <td>1116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Probabilistic-Programming-and-Bayesian-Methods...</td>\n",
       "      <td>21000</td>\n",
       "      <td>6600</td>\n",
       "      <td>1400</td>\n",
       "      <td>127</td>\n",
       "      <td>['bayesian-methods', 'pymc', 'mathematical-ana...</td>\n",
       "      <td>aka  bayesian methods for hackers   an introdu...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>https://github.com/CamDavidsonPilon/Probabilis...</td>\n",
       "      <td>jupyter notebook</td>\n",
       "      <td>1.72</td>\n",
       "      <td>[jupyter, notebook]</td>\n",
       "      <td>9459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data-Science--Cheat-Sheet</td>\n",
       "      <td>18400</td>\n",
       "      <td>8200</td>\n",
       "      <td>1500</td>\n",
       "      <td>7</td>\n",
       "      <td>[]</td>\n",
       "      <td>cheat sheets</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://github.com/abhat222/Data-Science--Chea...</td>\n",
       "      <td>no language</td>\n",
       "      <td>0.00</td>\n",
       "      <td>[no, language]</td>\n",
       "      <td>1439</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           repo_name   star   fork  watch  \\\n",
       "0                                              keras  47900  18100   2100   \n",
       "1                                       scikit-learn  40300  19600   2200   \n",
       "2                          PythonDataScienceHandbook  23100   9900   1500   \n",
       "3  Probabilistic-Programming-and-Bayesian-Methods...  21000   6600   1400   \n",
       "4                          Data-Science--Cheat-Sheet  18400   8200   1500   \n",
       "\n",
       "   issue                                               tags  \\\n",
       "0   2940  ['deep-learning', 'tensorflow', 'neural-networ...   \n",
       "1   1505  ['machine-learning', 'python', 'statistics', '...   \n",
       "2     65  ['scikit-learn', 'numpy', 'python', 'jupyter-n...   \n",
       "3    127  ['bayesian-methods', 'pymc', 'mathematical-ana...   \n",
       "4      7                                                 []   \n",
       "\n",
       "                                         description  contributers  license  \\\n",
       "0                           deep learning for humans          49.0        1   \n",
       "1           scikit learn  machine learning in python         108.0        1   \n",
       "2  python data science handbook  full text in jup...           0.0        1   \n",
       "3  aka  bayesian methods for hackers   an introdu...           0.0        1   \n",
       "4                                       cheat sheets           0.0        0   \n",
       "\n",
       "                                            repo_url    most_used_lang  \\\n",
       "0                https://github.com/keras-team/keras            python   \n",
       "1       https://github.com/scikit-learn/scikit-learn            python   \n",
       "2  https://github.com/jakevdp/PythonDataScienceHa...  jupyter notebook   \n",
       "3  https://github.com/CamDavidsonPilon/Probabilis...  jupyter notebook   \n",
       "4  https://github.com/abhat222/Data-Science--Chea...       no language   \n",
       "\n",
       "   tag_ratio             language  desc_to_num  \n",
       "0       9.31             [python]          737  \n",
       "1       5.97             [python]          477  \n",
       "2       2.42  [jupyter, notebook]         1116  \n",
       "3       1.72  [jupyter, notebook]         9459  \n",
       "4       0.00       [no, language]         1439  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above we can see that their are some numerical values and stopwords too, so we need to remove them from vocabulary list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['repo_name', 'star', 'fork', 'watch', 'issue', 'tags', 'description',\n",
       "       'contributers', 'license', 'repo_url', 'most_used_lang', 'tag_ratio',\n",
       "       'language', 'desc_to_num', 'purebasic', 'emacs', 'jupyter', 'no',\n",
       "       'assembly', 'matlab', 'viml', 'kotlin', 'groff', 'groovy', 'vue',\n",
       "       'scala', 'erlang', 'lisp', 'java', 'typescript', 'actionscript',\n",
       "       'shaderlab', 'scheme', 'ruby', 'pascal', 'swift', 'javascript',\n",
       "       'coffeescript', 'jsonnet', 'clojure', 'r', 'apex', 'objective', 'dart',\n",
       "       'html', 'script', 'arc', 'rust', 'lua', 'qml', 'php', 'dockerfile',\n",
       "       'perl', 'notebook', 'vhdl', 'powershell', 'vim', 'tex', 'c', 'pure',\n",
       "       'makefile', 'eagle', 'julia', 'css', 'data', 'shell', 'ragel', 'elixir',\n",
       "       'ocaml', 'racket', 'processing', 'go', 'language', 'python', 'roff',\n",
       "       'xslt', 'freemarker', 'io', 'other', 'hcl', 'haskell'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set = pd.concat([data, language_col],axis=1)\n",
    "training_set.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set.drop(['repo_name', 'tags', 'description', 'repo_url', 'language', 'no', 'most_used_lang'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_set = training_set.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LR can be used:\n",
    "# gradient descent\n",
    "# random forest \n",
    "# NN regression (Relu)\n",
    "# Lasso regression\n",
    "# Ridge regression\n",
    "# Elastic net regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_set.iloc[:,1:], training_set.iloc[:,0], test_size=0.2, random_state=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1200, 72)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.303941592424989e+16\n",
      "R2 score:  -4.2170038614981604e+24\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "sgd=SGDRegressor(loss='squared_loss',alpha=0.3,max_iter=5000)\n",
    "sgd.fit(X_train, y_train)\n",
    "\n",
    "predictions = sgd.predict(X_test)\n",
    "mse = mean_squared_error(y_test, predictions, squared=False)\n",
    "\n",
    "print('Mean Squared Error:', mse)\n",
    "accu=r2_score(y_test, predictions)\n",
    "print(\"R2 score: \", accu)\n",
    "\n",
    "if accu > ac:\n",
    "    model = sgd\n",
    "    ac = accu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normal Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5681.986682232555\n",
      "R2 score:  0.8752789669984791\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "predictions = lr.predict(X_test)\n",
    "mse = mean_squared_error(y_test, predictions, squared=False)\n",
    "\n",
    "print(\"Mean Squared Error: \", mse)\n",
    "accu=r2_score(y_test, predictions)\n",
    "print(\"R2 score: \",accu)\n",
    "\n",
    "if accu > ac:\n",
    "    model = lr\n",
    "    ac = accu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4801.792011241244\n",
      "R2 score:  0.9109270233630825\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf = RandomForestRegressor(max_depth=8, random_state=25)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "predictions = rf.predict(X_test)\n",
    "mse = mean_squared_error(y_test, predictions, squared=False)\n",
    "\n",
    "print(\"Mean Squared Error: \", mse)\n",
    "accu=r2_score(y_test, predictions)\n",
    "print(\"R2 score: \",accu)\n",
    "\n",
    "if accu > ac:\n",
    "    model = rf\n",
    "    ac = accu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5680.798186828632\n",
      "R2 score:  0.8753311370862266\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "las = linear_model.Lasso(alpha=0.3,max_iter=3000)\n",
    "las.fit(X_train, y_train)\n",
    "\n",
    "predictions = las.predict(X_test)\n",
    "mse = mean_squared_error(y_test, predictions, squared=False)\n",
    "\n",
    "print(\"Mean Squared Error: \", mse)\n",
    "accu=r2_score(y_test, predictions)\n",
    "print(\"R2 score: \",accu)\n",
    "\n",
    "if accu > ac:\n",
    "    model = las\n",
    "    ac = accu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5689.720420421575\n",
      "R2 score:  0.874939220955455\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "las = linear_model.Ridge(alpha=0.3,max_iter=3000)\n",
    "las.fit(X_train, y_train)\n",
    "\n",
    "predictions = las.predict(X_test)\n",
    "mse = mean_squared_error(y_test, predictions, squared=False)\n",
    "\n",
    "print(\"Mean Squared Error: \", mse)\n",
    "accu=r2_score(y_test, predictions)\n",
    "print(\"R2 score: \",accu)\n",
    "\n",
    "if accu > ac:\n",
    "    model = las\n",
    "    ac = accu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elastic Net Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5848.999716530464\n",
      "R2 score:  0.8678392553627553\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "elsnr = ElasticNet(alpha=0.03,max_iter=2000)\n",
    "elsnr.fit(X_train, y_train)\n",
    "\n",
    "predictions = elsnr.predict(X_test)\n",
    "mse = mean_squared_error(y_test, predictions, squared=False)\n",
    "\n",
    "print(\"Mean Squared Error: \", mse)\n",
    "accu=r2_score(y_test, predictions)\n",
    "print(\"R2 score: \",accu)\n",
    "\n",
    "if accu > ac:\n",
    "    model = elsnr\n",
    "    ac = accu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All results:\n",
    "\n",
    "| Methods                | MSE      | R2 Score |\n",
    "|------------------------|----------|----------|\n",
    "| Gradient Descent         | 3.303    | -4.217 |\n",
    "| Linear Regression        | 5681.986 | 0.875  |\n",
    "| Random Forest            | 4801.79  | 0.910  |\n",
    "| Lasso Regression         | 5680.798 | 0.8753 |\n",
    "| Ridge Regression         | 5689.72  | 0.8749 |\n",
    "| Elastic Net Regression   | 5848.99  | 0.868  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above table, we can see that the `Random Forest` perform better than others methods with 0.91 r2 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from numpy import absolute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# xg = XGBRegressor(verbosity=1, eta=0.2)\n",
    "# params = {'min_child_weight':[4,5], 'gamma':[i/10.0 for i in range(3,6)],  'subsample':[i/10.0 for i in range(6,11)],\n",
    "# 'colsample_bytree':[i/10.0 for i in range(6,11)], 'max_depth': [4,6,7]}\n",
    "# gb = GridSearchCV(xg,params)\n",
    "# gb.fit(X_train, y_train)\n",
    "\n",
    "# predictions = gb.predict(X_test)\n",
    "# mse = mean_squared_error(y_test, predictions, squared=False)\n",
    "\n",
    "# print(\"Mean Squared Error: \", mse)\n",
    "# accu=r2_score(y_test, predictions)\n",
    "# print(\"R2 score: \", accu)\n",
    "\n",
    "# if accu > ac:\n",
    "#     model = gb\n",
    "#     ac = accu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model in `pickle` format\n",
    "with open('../pretrained_model/train_model.pkl', 'wb') as f:  \n",
    "    pickle.dump(model, f)\n",
    "with open('../pretrained_model/objs.pkl', 'wb') as f:  \n",
    "    pickle.dump([store_tag, vocabulary, vocab, word_dict], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the model on the repo to predict its popularity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_name=input(\"Enter username:\")\n",
    "repo_name=input(\"Enter repo name:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "your repo Popularity: 35\n"
     ]
    }
   ],
   "source": [
    "header={'Accept':'application/vnd.github.mercy-preview+json',\n",
    "'visibility':'PUBLIC',\n",
    "\"Authorization\": \"token PASTE_YOUR_GITHUB_TOKEN\",\n",
    "}\n",
    "\n",
    "request=requests.get('https://api.github.com/repos/'+user_name+'/'+repo_name,headers=header)\n",
    "data=request.json()\n",
    "keys=['forks','watchers','open_issues_count','topics','description','language']\n",
    "\n",
    "dataf=[data[k] for k in keys]\n",
    "if str(data['license']).lower()=='none' or data['license']:\n",
    "    license=0\n",
    "else:\n",
    "    license=1\n",
    "\n",
    "request=requests.get('https://api.github.com/repos/'+user_name+'/'+repo_name+'contributors',headers=header)\n",
    "dat=request.json()\n",
    "contributors=len(dat)\n",
    "dataf.append(contributors)\n",
    "dataf.append(license)\n",
    "df=pd.DataFrame([dataf],columns=['fork','watch','issue','tags','description','most_used_lang','contributers','license'])\n",
    "def perc(tags):\n",
    "    sum_of_perc=0\n",
    "    for tag in tags:\n",
    "        if tag in store_tag:\n",
    "            sum_of_perc+=(store_tag[tag]/vocabulary)\n",
    "\n",
    "    return (sum_of_perc*100)\n",
    "\n",
    "df['tag_ratio']=df['tags'].apply(perc).astype(float)\n",
    "text=nltk.word_tokenize(df['description'][0])\n",
    "tex=nltk.word_tokenize(repo_name)\n",
    "\n",
    "def string_num(text):\n",
    "    string_numbers=0\n",
    "    for string in text:\n",
    "        if string in word_dict:\n",
    "            string_numbers+=word_dict[string]\n",
    "    return string_numbers\n",
    "\n",
    "df['desc_to_num']=string_num(text)\n",
    "\n",
    "word_counts_for_language = {unique_word: [0] for unique_word in vocab}\n",
    "\n",
    "df['most_used_lang']=df['most_used_lang'].str.split()\n",
    "for word in df['most_used_lang']:\n",
    "    if word[0] in word_counts_for_language:\n",
    "        word_counts_for_language[word[0].lower()][0] += 1\n",
    "\n",
    "language_col=pd.DataFrame(word_counts_for_language)\n",
    "test_set=pd.concat([df,language_col],axis=1)\n",
    "test_set.drop(['tags','description','language','no','most_used_lang'], axis=1, inplace=True)\n",
    "\n",
    "print(\"Your repo Popularity:\",int(model.predict(test_set)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
